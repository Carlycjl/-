{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:25:42.207217Z",
     "start_time": "2024-05-30T02:25:41.707237Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "## Input data directory\n",
    "data_dir = \"cureus\"\n",
    "inputdirectory = Path(f\"./data_input/{data_dir}\")\n",
    "## This is where the output csv files will be written\n",
    "out_dir = data_dir\n",
    "outputdirectory = Path(f\"./data_output1/{out_dir}\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:25:44.107125Z",
     "start_time": "2024-05-30T02:25:42.208216Z"
    }
   },
   "source": [
    "## Dir PDF Loader\n",
    "# loader = PyPDFDirectoryLoader(inputdirectory)\n",
    "## File Loader\n",
    "# loader = PyPDFLoader(\"./data/MedicalDocuments/orf-path_health-n1.pdf\")\n",
    "loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "pages = splitter.split_documents(documents)\n",
    "print(\"Number of chunks = \", len(pages))\n",
    "print(pages[3].page_content)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks =  5\n",
      "（二）排污单位依法终止的；\n",
      "\n",
      "（三）排污许可证依法被撤销、吊销的；\n",
      "\n",
      "（四）应当注销的其他情形。\n",
      "\n",
      "第三十条  有下列情形之一的，可以依法撤销排污许可证，并在全国排污许可证管理信息平台上公告：\n",
      "\n",
      "（一）超越法定职权审批排污许可证的；\n",
      "\n",
      "（二）违反法定程序审批排污许可证的；\n",
      "\n",
      "（三）审批部门工作人员滥用职权、玩忽职守审批排污许可证的；\n",
      "\n",
      "（四）对不具备申请资格或者不符合法定条件的排污单位审批排污许可证的；\n",
      "\n",
      "（五）依法可以撤销排污许可证的其他情形。\n",
      "\n",
      "排污单位以欺骗、贿赂等不正当手段取得排污许可证的，应当依法予以撤销。\n",
      "\n",
      "第三十一条  上级生态环境主管部门可以对具有审批权限的下级生态环境主管部门的排污许可证审批和执行情况进行监督检查和指导，发现属于《条例》第三十二条规定违法情形的，上级生态环境主管部门应当责令改正。\n",
      "\n",
      "第三十二条  排污许可证发生遗失、损毁的，排污单位可以向审批部门申请补领。已经办理排污许可证电子证照的排污单位可以根据需要自行打印排污许可证。\n",
      "\n",
      "第四章  排污管理\n",
      "\n",
      "第三十三条  排污单位应当依照《条例》规定，严格落实环境保护主体责任，建立健全环境管理制度，按照排污许可证规定严格控制污染物排放。\n",
      "\n",
      "排污登记单位应当依照国家生态环境保护法律法规规章等管理规定运行和维护污染防治设施，建设规范化排放口，落实排污主体责任，控制污染物排放。\n",
      "\n",
      "第三十四条  排污单位应当按照排污许可证规定和有关标准规范，依法开展自行监测，保存原始监测记录。原始监测记录保存期限不得少于五年。\n",
      "\n",
      "排污单位对自行监测数据的真实性、准确性负责，不得篡改、伪造。\n",
      "\n",
      "第三十五条  实行排污许可重点管理的排污单位，应当依法安装、使用、维护污染物排放自动监测设备，并与生态环境主管部门的监控设备联网。\n",
      "\n",
      "排污单位发现污染物排放自动监测设备传输数据异常的，应当及时报告生态环境主管部门，并进行检查、修复。\n",
      "\n",
      "第三十六条  排污单位应当按照排污许可证规定的格式、内容和频次要求记录环境管理台账，主要包括以下内容：\n",
      "\n",
      "（一）与污染物排放相关的主要生产设施运行情况；发生异常情况的，应当记录原因和采取的措施。\n",
      "\n",
      "（二）污染防治设施运行情况及管理信息；发生异常情况的，应当记录原因和采取的措施。\n",
      "\n",
      "（三）污染物实际排放浓度和排放量；发生超标排放情况的，应当记录超标原因和采取的措施。\n",
      "\n",
      "（四）其他按照相关技术规范应当记录的信息。\n",
      "\n",
      "环境管理台账记录保存期限不得少于五年。\n",
      "\n",
      "第三十七条  排污单位应当按照排污许可证规定的执行报告内容、频次和时间要求，在全国排污许可证管理信息平台上填报、提交排污许可证执行报告。\n",
      "\n",
      "排污许可证执行报告包括年度执行报告、季度执行报告和月执行报告。\n",
      "\n",
      "季度执行报告和月执行报告应当包括以下内容：\n",
      "\n",
      "（一）根据自行监测结果说明污染物实际排放浓度和排放量及达标判定分析；\n",
      "\n",
      "（二）排污单位超标排放或者污染防治设施异常情况的说明。\n",
      "\n",
      "年度执行报告可以替代当季度或者当月的执行报告，并增加以下内容：\n",
      "\n",
      "（一）排污单位基本生产信息；\n",
      "\n",
      "（二）污染防治设施运行情况；\n",
      "\n",
      "（三）自行监测执行情况；\n",
      "\n",
      "（四）环境管理台账记录执行情况；\n",
      "\n",
      "（五）信息公开情况；\n",
      "\n",
      "（六）排污单位内部环境管理体系建设与运行情况；\n",
      "\n",
      "（七）其他排污许可证规定的内容执行情况。\n",
      "\n",
      "建设项目竣工环境保护设施验收报告中污染源监测数据等与污染物排放相关的主要内容，应当由排污单位记载在该项目竣工环境保护设施验收完成当年的排污许可证年度执行报告中。排污许可证执行情况应当作为环境影响后评价的重要依据。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe of all the chunks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:25:44.145931Z",
     "start_time": "2024-05-30T02:25:44.107125Z"
    }
   },
   "source": [
    "from helpers.df_helpers import documents2Dataframe\n",
    "df = documents2Dataframe(pages)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  排污许可管理办法\\n\\n（2024年4月1日生态环境部令第32号公布，自2024年7月1日起...   \n",
       "1  第十二条  排污单位承诺执行更加严格的排放限值的，应当在排污许可证副本中记载。\\n\\n第十三...   \n",
       "2  第二十二条  对具备下列条件的排污单位，颁发排污许可证：\\n\\n（一）依法取得建设项目环境影...   \n",
       "3  （二）排污单位依法终止的；\\n\\n（三）排污许可证依法被撤销、吊销的；\\n\\n（四）应当注销...   \n",
       "4  （七）其他排污许可证规定的内容执行情况。\\n\\n建设项目竣工环境保护设施验收报告中污染源监测...   \n",
       "\n",
       "                                         source  \\\n",
       "0  data_input\\cureus\\P020240408373065009150.txt   \n",
       "1  data_input\\cureus\\P020240408373065009150.txt   \n",
       "2  data_input\\cureus\\P020240408373065009150.txt   \n",
       "3  data_input\\cureus\\P020240408373065009150.txt   \n",
       "4  data_input\\cureus\\P020240408373065009150.txt   \n",
       "\n",
       "                           chunk_id  \n",
       "0  26a229efdbdc4edd981b7dfcb1f56fcb  \n",
       "1  897f1388a9714c26917587712d099192  \n",
       "2  b022491c4f69470294dafbdc770576bb  \n",
       "3  8fb7334fcd194391a1e869a1121a896e  \n",
       "4  09a9d4bfc9684ba78f6aef52f9e2e241  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>排污许可管理办法\\n\\n（2024年4月1日生态环境部令第32号公布，自2024年7月1日起...</td>\n",
       "      <td>data_input\\cureus\\P020240408373065009150.txt</td>\n",
       "      <td>26a229efdbdc4edd981b7dfcb1f56fcb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>第十二条  排污单位承诺执行更加严格的排放限值的，应当在排污许可证副本中记载。\\n\\n第十三...</td>\n",
       "      <td>data_input\\cureus\\P020240408373065009150.txt</td>\n",
       "      <td>897f1388a9714c26917587712d099192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>第二十二条  对具备下列条件的排污单位，颁发排污许可证：\\n\\n（一）依法取得建设项目环境影...</td>\n",
       "      <td>data_input\\cureus\\P020240408373065009150.txt</td>\n",
       "      <td>b022491c4f69470294dafbdc770576bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>（二）排污单位依法终止的；\\n\\n（三）排污许可证依法被撤销、吊销的；\\n\\n（四）应当注销...</td>\n",
       "      <td>data_input\\cureus\\P020240408373065009150.txt</td>\n",
       "      <td>8fb7334fcd194391a1e869a1121a896e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>（七）其他排污许可证规定的内容执行情况。\\n\\n建设项目竣工环境保护设施验收报告中污染源监测...</td>\n",
       "      <td>data_input\\cureus\\P020240408373065009150.txt</td>\n",
       "      <td>09a9d4bfc9684ba78f6aef52f9e2e241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Concepts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:25:44.149190Z",
     "start_time": "2024-05-30T02:25:44.146936Z"
    }
   },
   "source": [
    "## This function uses the helpers/prompt function to extract concepts from text\n",
    "from helpers.df_helpers import df2Graph\n",
    "from helpers.df_helpers import graph2Df"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If regenerate is set to True then the dataframes are regenerated and Both the dataframes are written in the csv format so we dont have to calculate them again. \n",
    "\n",
    "        dfne = dataframe of edges\n",
    "\n",
    "        df = dataframe of chunks\n",
    "\n",
    "\n",
    "Else the dataframes are read from the output directory"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:26:39.332291Z",
     "start_time": "2024-05-30T02:26:20.729484Z"
    }
   },
   "cell_type": "code",
   "source": "concepts_list = df2Graph(df, model='qwen:4b')",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"node_1\": \"A concept from extracted ontology\",\n",
      "    \"node_2\": \"A related concept from extracted ontology\",\n",
      "    \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\n",
      "}\n",
      "]\n",
      "\n",
      "ERROR ### Here is the buggy response:  {\n",
      "    \"node_1\": \"A concept from extracted ontology\",\n",
      "    \"node_2\": \"A related concept from extracted ontology\",\n",
      "    \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\n",
      "}\n",
      "] \n",
      "\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"A concept from extracted ontology\", \n",
      "        \"node_2\": \"A related concept from extracted ontology\", \n",
      "        \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\" \n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "ERROR ### Here is the buggy response:  ```json\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"A concept from extracted ontology\", \n",
      "        \"node_2\": \"A related concept from extracted ontology\", \n",
      "        \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\" \n",
      "    }\n",
      "]\n",
      "``` \n",
      "\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"概念\", \n",
      "        \"node_2\": \"关系\", \n",
      "        \"edge\": \"关系概念\" \n",
      "    }, \n",
      "    { ... } \n",
      "]\n",
      "```\n",
      "\n",
      "output:\n",
      "\n",
      "ERROR ### Here is the buggy response:  ```json\n",
      "[\n",
      "    {\n",
      "        \"node_1\": \"概念\", \n",
      "        \"node_2\": \"关系\", \n",
      "        \"edge\": \"关系概念\" \n",
      "    }, \n",
      "    { ... } \n",
      "]\n",
      "```\n",
      "\n",
      "output: \n",
      "\n",
      "\n",
      "{\n",
      "    \"node_1\": \"A concept from extracted ontology\", \n",
      "    \"node_2\": \"A related concept from extracted ontology\", \n",
      "    \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\" \n",
      "}\n",
      "```\n",
      "\n",
      "ERROR ### Here is the buggy response:  {\n",
      "    \"node_1\": \"A concept from extracted ontology\", \n",
      "    \"node_2\": \"A related concept from extracted ontology\", \n",
      "    \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\" \n",
      "}\n",
      "``` \n",
      "\n",
      "\n",
      "抱歉，您的问题没有足够的信息进行回答。如果您有任何其他的问题，欢迎随时向我提问，我会尽力为您解答。再次感谢您对我耐心的帮助和支持，我会继续努力学习不断提高自己的能力素质水平，为社会贡献自己的一份力量，帮助更多需要帮助的人。再次感谢您对我耐心的帮助和支持，我会继续努力学习不断提高自己的能力素质水平，为社会贡献自己的一份力量，帮助更多需要帮助的人。\n",
      "\n",
      "ERROR ### Here is the buggy response:  抱歉，您的问题没有足够的信息进行回答。如果您有任何其他的问题，欢迎随时向我提问，我会尽力为您解答。再次感谢您对我耐心的帮助和支持，我会继续努力学习不断提高自己的能力素质水平，为社会贡献自己的一份力量，帮助更多需要帮助的人。再次感谢您对我耐心的帮助和支持，我会继续努力学习不断提高自己的能力素质水平，为社会贡献自己的一份力量，帮助更多需要帮助的人。 \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m concepts_list \u001B[38;5;241m=\u001B[39m \u001B[43mdf2Graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mqwen:4b\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\workspace\\Eco_LLM\\kg\\knowledge_graph-main\\helpers\\df_helpers.py:60\u001B[0m, in \u001B[0;36mdf2Graph\u001B[1;34m(dataframe, model)\u001B[0m\n\u001B[0;32m     57\u001B[0m results \u001B[38;5;241m=\u001B[39m results\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m## Flatten the list of lists to one single list of entities.\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m concept_list \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mravel()\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concept_list\n",
      "\u001B[1;31mValueError\u001B[0m: need at least one array to concatenate"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T01:45:08.501683Z",
     "start_time": "2024-05-30T01:45:08.496773Z"
    }
   },
   "cell_type": "code",
   "source": "df['text'][5]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'第二十条  审批部门收到排污单位提交的申请材料后，依照《条例》第九条、第十条要求作出处理。\\n\\n审批部门可以组织技术机构对排污许可证申请材料进行技术评估，并承担相应费用。技术机构应当遵循科学、客观、公正的原则，提出技术评估意见，并对技术评估意见负责，不得向排污单位收取任何费用。\\n\\n技术机构开展技术评估应当遵守国家相关法律法规、标准规范，保守排污单位商业秘密。\\n\\n第二十一条  排污单位采用相应污染防治可行技术的，或者新建、改建、扩建建设项目排污单位采用环境影响报告书（表）批准文件要求的污染防治技术的，审批部门可以认为排污单位采用的污染防治设施或者措施能够达到许可排放浓度要求。\\n\\n不符合前款规定情形的，排污单位可以通过提供监测数据证明其采用的污染防治设施可以达到许可排放浓度要求。监测数据应当通过使用符合国家有关环境监测、计量认证规定和技术规范的监测设备取得；对于国内首次采用的污染防治技术，应当提供工程试验数据予以证明。\\n\\n第二十二条  对具备下列条件的排污单位，颁发排污许可证：\\n\\n（一）依法取得建设项目环境影响报告书（表）批准文件，或者已经办理环境影响登记表备案手续；'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## To regenerate the graph with LLM, set this to True\n",
    "regenerate = True\n",
    "\n",
    "if regenerate:\n",
    "    concepts_list = df2Graph(df, model='qwen:4b')\n",
    "    dfg1 = graph2Df(concepts_list)\n",
    "    if not os.path.exists(outputdirectory):\n",
    "        os.makedirs(outputdirectory)\n",
    "    \n",
    "    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "else:\n",
    "    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "\n",
    "dfg1.replace(\"\", np.nan, inplace=True)\n",
    "dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "dfg1['count'] = 4 \n",
    "## Increasing the weight of the relation to 4. \n",
    "## We will assign the weight of 1 when later the contextual proximity will be calculated.  \n",
    "print(dfg1.shape)\n",
    "dfg1.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating contextual proximity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)\n",
    "    # Self join with chunk id as the key will create a link between terms occuring in the same text chunk.\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
    "    # drop self loops\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "    ## Group and count edges.\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n",
    "        .reset_index()\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    # Drop edges with 1 count\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"\n",
    "    return dfg2\n",
    "\n",
    "\n",
    "dfg2 = contextual_proximity(dfg1)\n",
    "dfg2.tail()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge both the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dfg = pd.concat([dfg1, dfg2], axis=0)\n",
    "dfg = (\n",
    "    dfg.groupby([\"node_1\", \"node_2\"])\n",
    "    .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "dfg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the NetworkX Graph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "nodes.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "## Add nodes to the graph\n",
    "for node in nodes:\n",
    "    G.add_node(\n",
    "        str(node)\n",
    "    )\n",
    "\n",
    "## Add edges to the graph\n",
    "for index, row in dfg.iterrows():\n",
    "    G.add_edge(\n",
    "        str(row[\"node_1\"]),\n",
    "        str(row[\"node_2\"]),\n",
    "        title=row[\"edge\"],\n",
    "        weight=row['count']/4\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate communities for coloring the nodes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "communities_generator = nx.community.girvan_newman(G)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)\n",
    "communities = sorted(map(sorted, next_level_communities))\n",
    "print(\"Number of Communities = \", len(communities))\n",
    "print(communities)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe for community colors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "palette = \"hls\"\n",
    "\n",
    "## Now add these colors to communities and make another dataframe\n",
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()\n",
    "    random.shuffle(p)\n",
    "    rows = []\n",
    "    group = 0\n",
    "    for community in communities:\n",
    "        color = p.pop()\n",
    "        group += 1\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
    "    df_colors = pd.DataFrame(rows)\n",
    "    return df_colors\n",
    "\n",
    "\n",
    "colors = colors2Community(communities)\n",
    "colors"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add colors to the graph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for index, row in colors.iterrows():\n",
    "    G.nodes[row['node']]['group'] = row['group']\n",
    "    G.nodes[row['node']]['color'] = row['color']\n",
    "    G.nodes[row['node']]['size'] = G.degree[row['node']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "graph_output_directory = \"./docs/index.html\"\n",
    "\n",
    "net = Network(\n",
    "    notebook=True,\n",
    "    # bgcolor=\"#1a1a1a\",\n",
    "    cdn_resources=\"remote\",\n",
    "    height=\"900px\",\n",
    "    width=\"100%\",\n",
    "    select_menu=True,\n",
    "    # font_color=\"#cccccc\",\n",
    "    filter_menu=False,\n",
    ")\n",
    "\n",
    "net.from_nx(G)\n",
    "# net.repulsion(node_distance=150, spring_length=400)\n",
    "net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n",
    "# net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n",
    "net.show_buttons(filter_=[\"physics\"])\n",
    "\n",
    "net.show(graph_output_directory)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text_kg)",
   "language": "python",
   "name": "text_kg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
